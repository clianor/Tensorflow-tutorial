안녕하십니까 록스에 이철희 입니다

강화학습이라는 개념이 나온 지는 벌써 오래 되었지만 최근에 중요한 머신러닝의 페러다임이 자리를 잡고 나서야 강화학습이 비로서 그 진가를 드러내게 되었습니다. 아타리게임을 사람보다 잘하고 바둑으로 이세돌 9단을 이기는 등 괄목할만한 성과를 주고 있는 알고리즘이 강화학습입니다.

일반적으로 머신러닝에는 3가지로 분류하는데 지도학습, 비지도학습, 그리고 오늘 배울 강화학습입니다.

지도학습 :
비지도학습 :
먼저 강화학습의 정의를 보겠습니다. 위키피디아를 보면

강화 학습(Reinforcement learning)은 기계학습이 다루는 문제 중에서 다음과 같이 기술 되는 것을 다룬다. 어떤 환경을 탐색하는 에이전트가 현재의 상태를 인식하여 어떤 행동을 취한다. 그러면 그 에이전트는 환경으로부터 포상을 얻게 된다. 포상은 양수와 음수 둘 다 가능하다. 강화 학습의 알고리즘은 그 에이전트가 앞으로 누적될 포상을 최대화하는 일련의 행동으로 정의되는 정책을 찾는 방법이다.

위에것을 해석해 보면

강화학습은 상호 관계에 대한 고려가 부족했습니다. 상호 관계에 바탕을 둔 강화학습은 에이전트(행위자)의 액션(행위)은 환경에 영향을 줍니다.

강화학습은 강화이론을 머신러닝에 적용한 형태라고 이해하면 되는데 알고리즘에 따라서 어떤 판단을 했을 때 돌아오는 reward 에 따라서 이 판단이 옳은 것이었는지를 학습하는 것이다.

강화학습을 진행할 때는 크게 state, action, reward 에 신경을 쓰면 됩니다. state 는 현재의 상태를, action 은 현재 state 에 기반하여 다음에 취하게 될 동작을 나타낸다. reward 는 action 에 따라 업게 되는 보상이다. reward 는 각 action 마다 특정한 값으로 보상이 되는데, 양수/음수/0의 값으로 돌아온다.

생각할 수 있는 그 어떤 과제든 MDP로 표현할 수 있습니다. 예를들어 문을 여는 액션을 상상해봅니다.

상태는 우리몸의 위치, 그 세계에 존재하는 문, 그리고 우리가 그 문을 시각적으로 어떻게 바라보고 있는지 등이 될 수 있습니다.
액션은 우리 몸이 만들어내는 모든 동작이 됩니다.
보상은 성공적으로 열린 문
비록 문으로 다가가는 등의 동작은 문제 해결에 필수적인 액션이기는 하지만, 실제로 문을 여는 액션만이 보상을 제공하기 때문에 그 자체로 보상이 주어지는 액션은 아닙니다. 이 경우 에이전트는 최종적으로 보상으로 이끄는 액션에 가치를 할당하는 법을 학습할 필요가 있기 떄문에 시간의 경과에 따른 역학개념이 필요합니다.